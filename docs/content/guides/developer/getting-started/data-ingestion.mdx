---
title: Data ingestion
---

Sui provides a micro data ingestion framework that allows building custom indexers. It works by subscribing to a checkpoint stream with full checkpoint content.

## Interface and Data format

In order to use the framework, a simple interface needs to be implemented:
```rust
#[async_trait]
trait Worker: Send + Sync {
    async fn process_checkpoint(&self, checkpoint: CheckpointData) -> Result<()>;
}
```

`CheckpointData` struct represents full checkpoint content. It contains checkpoint summary, checkpoint contents, as well as detailed information about each individual transaction, including events and input/output objects. Full definition can be found [here](https://github.com/MystenLabs/sui/blob/releases/sui-graphql-rpc-v2024.1.0-release/crates/sui-types/src/full_checkpoint_content.rs).


## Checkpoint stream sources
Data ingestion supports several checkpoint stream sources.
#### Remote reader
The easiest way to start is to subscribe to a remote store of checkpoint contents. Mysten Labs provides the following buckets:
- Testnet: `https://checkpoints.testnet.sui.io`
- Mainnet: `https://checkpoints.mainnet.sui.io`

#### Local reader
Another way is to colocate the data ingestion daemon with a full node and enable checkpoint dumping on the latter. Once enabled, the full node will start dumping executed checkpoints as files to a local directory, and the data ingestion daemon will subscribe to changes in the directory via an inotify-like mechanism. This approach allows minimizing ingestion latency (checkpoint will be processed immediately after a checkpoint executor on a full node) and getting rid of dependency on an externally managed bucket.

To enable such a setup, add the following configuration to a full node:

```yaml
checkpoint-executor-config:
  checkpoint-execution-max-concurrency: 200
  local-execution-timeout-sec: 30
  data-ingestion-dir: <path to a local directory>
```

#### Hybrid mode
Developers can also specify both a local and remote store as a fallback. The framework always prioritizes locally available checkpoint data over remote one. It's useful when you want to start utilizing your own full node for data ingestion but need to partially backfill historical data or just have a failover.

## Example

Here's a basic example of a custom data ingestion worker.

```rust
struct CustomWorker;

#[async_trait]
impl Worker for CustomWorker {
    async fn process_checkpoint(&self, checkpoint: CheckpointData) -> Result<()> {
        // custom processing logic
        ...
        Ok(())
    }
}

#[tokio::main]
async fn main() -> Result<()> {
  let (exit_sender, exit_receiver) = oneshot::channel();
  let metrics = DataIngestionMetrics::new(&Registry::new());
  let progress_store = FileProgressStore::new("path_to_file");
  let mut executor = IndexerExecutor::new(progress_store, 1 /* number of workflow types */, metrics);
  let worker_pool = WorkerPool::new(CustomWorker, "custom worker", 100);
  executor.register(worker_pool).await?;
  executor.run(
      PathBuf::from("..."), // path to a local directory
      Some("https://s3.us-west-2.amazonaws.com/mysten-mainnet-checkpoints".to_string()),
      vec![], // optional remote store access options
      exit_receiver,
  ).await?;
  Ok(())
}
```

Let's highlight a couple of code blocks:
```rust
let worker_pool = WorkerPool::new(CustomWorker, "custom worker", 100);
executor.register(worker_pool).await?;
```

The data ingestion executor can run multiple workflows simultaneously. For each workflow, a separate worker pool needs to be created and registered in the executor. The WorkerPool requires an instance of the Worker trait, the name of the workflow (which is used for tracking the progress of the flow in the progress store and metrics), and concurrency.

The concurrency parameter specifies how many threads are used by the workflow. Having a concurrency value greater than 1 is helpful when tasks are idempotent and can be processed in parallel and out of order. Note that the executor only updates progress/watermark to a certain checkpoint when all preceding checkpoints are processed.

More examples of custom ingestion pipelines can be found in the [repository](https://github.com/MystenLabs/sui/tree/main/crates/sui-data-ingestion/src/workers) 

